{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyOkmgogjKzNTJ689wBMBVLm",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/arjunsn-03/AI-youtube-summarizer/blob/main/AI_Youtube_Video_Summarizer.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YNy-s-X16iW9",
        "outputId": "d5e94bcf-8e1a-4a88-d66f-bd388232db61"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m175.9/175.9 kB\u001b[0m \u001b[31m12.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m3.2/3.2 MB\u001b[0m \u001b[31m115.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "!pip install -q pytube # To download YouTube videos\n",
        "!pip install -q git+https://github.com/openai/whisper.git  # OpenAI Whisper ASR\n",
        "!pip install -q moviepy # For audio extraction (reliable, easy to use)\n",
        "!pip install -q yt-dlp\n",
        "# Install the Google GenAI SDK\n",
        "!pip install -q google-genai\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Audio Extraction from video"
      ],
      "metadata": {
        "id": "LOO_d2dN8MDn"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "YzrEIBtv-Mvf"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Colab Code Cell 2: Audio Extraction (FIXED)\n",
        "\n",
        "import os\n",
        "import subprocess\n",
        "\n",
        "# --- USER INPUT ---\n",
        "youtube_url = \"https://www.youtube.com/watch?v=6M5VXKLf4D4\" # Your video URL\n",
        "output_audio_file = \"extracted_audio.mp3\"\n",
        "\n",
        "# --- NEW DOWNLOAD AND EXTRACTION LOGIC USING YT-DLP ---\n",
        "print(f\"Downloading and extracting audio from: {youtube_url}\")\n",
        "\n",
        "# Define yt-dlp options to extract the best quality audio and convert it to MP3\n",
        "ydl_opts = [\n",
        "    youtube_url,\n",
        "    '-x',  # Command to extract audio only\n",
        "    '--audio-format', 'mp3',\n",
        "    '--audio-quality', '0',  # '0' means best possible quality\n",
        "    '-o', output_audio_file, # Define the output filename\n",
        "    '--force-overwrites'\n",
        "]\n",
        "\n",
        "try:\n",
        "    # Execute the yt-dlp command using the subprocess module\n",
        "    subprocess.run(['yt-dlp', *ydl_opts], check=True)\n",
        "    print(f\"‚úÖ Audio extraction complete. File: {output_audio_file}\")\n",
        "\n",
        "except subprocess.CalledProcessError as e:\n",
        "    print(f\"üö® Error during yt-dlp execution: {e}\")\n",
        "    print(\"This might indicate an issue with the video URL or network connection.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "shnrO3J08S5D",
        "outputId": "1ece8400-b7b7-4720-c6a4-28989b2fb081"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading and extracting audio from: https://www.youtube.com/watch?v=6M5VXKLf4D4\n",
            "‚úÖ Audio extraction complete. File: extracted_audio.mp3\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 2: High-Fidelity Transcription with Whisper\n",
        "This step uses the powerful Whisper model to transcribe the audio and provides time-stamps for everything.\n",
        "\n",
        "### Transcription"
      ],
      "metadata": {
        "id": "fn3T4uoGAMc0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import whisper\n",
        "import torch\n",
        "import json\n",
        "\n",
        "# Check for GPU\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "print(f\"Using device: {device}\")\n",
        "\n",
        "# Load the model. 'base' is fast and good; 'medium' is slower but more accurate.\n",
        "# Use 'base' for speed, 'medium' for higher accuracy.\n",
        "model_size = \"medium\"\n",
        "print(f\"Loading Whisper model: {model_size}...\")\n",
        "model = whisper.load_model(model_size, device=device)\n",
        "\n",
        "# Run the transcription\n",
        "print(\"Starting transcription...\")\n",
        "result = model.transcribe(\n",
        "    output_audio_file,\n",
        "    verbose=True,\n",
        "    word_timestamps=False  # Keep segment-level timestamps for faster processing\n",
        ")\n",
        "\n",
        "# Extract the segments\n",
        "transcript_segments = result[\"segments\"]\n",
        "full_text = result[\"text\"]\n",
        "\n",
        "print(\"\\n--- FIRST 5 SEGMENTS ---\")\n",
        "for i in range(min(5, len(transcript_segments))):\n",
        "    seg = transcript_segments[i]\n",
        "    print(f\"[{seg['start']:.2f} - {seg['end']:.2f}]: {seg['text'].strip()}\")\n",
        "\n",
        "print(f\"\\n‚úÖ Transcription complete. Total segments: {len(transcript_segments)}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WfOX1llc92Yx",
        "outputId": "9a12caf6-f440-4559-ca9a-5fd349e6b19f"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: cuda\n",
            "Loading Whisper model: medium...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1.42G/1.42G [00:36<00:00, 41.3MiB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Starting transcription...\n",
            "Detecting language using up to the first 30 seconds. Use `--language` to specify the language\n",
            "Detected language: English\n",
            "[00:00.000 --> 00:05.800]  Ever wondered how Google translates an entire web page to a different language in a matter of seconds?\n",
            "[00:05.800 --> 00:09.700]  Or your phone gallery group's images based on their location?\n",
            "[00:09.700 --> 00:12.400]  All of this is a product of deep learning.\n",
            "[00:12.400 --> 00:15.300]  But what exactly is deep learning?\n",
            "[00:15.300 --> 00:21.100]  Deep learning is a subset of machine learning, which in turn is a subset of artificial intelligence.\n",
            "[00:21.100 --> 00:26.400]  Artificial intelligence is a technique that enables a machine to mimic human behavior.\n",
            "[00:26.500 --> 00:31.600]  Machine learning is a technique to achieve AI through algorithms trained with data.\n",
            "[00:31.600 --> 00:37.900]  And finally, deep learning is a type of machine learning inspired by the structure of the human brain.\n",
            "[00:37.900 --> 00:42.800]  In terms of deep learning, this structure is called an artificial neural network.\n",
            "[00:42.800 --> 00:47.300]  Let's understand deep learning better and how it's different from machine learning.\n",
            "[00:47.300 --> 00:51.900]  Say we create a machine that could differentiate between tomatoes and cherries.\n",
            "[00:51.900 --> 00:58.600]  If done using machine learning, we'd have to tell the machine the features based on which the two can be differentiated.\n",
            "[00:58.600 --> 01:02.300]  These features could be the size and the type of stem on them.\n",
            "[01:02.300 --> 01:08.700]  With deep learning, on the other hand, the features are picked out by the neural network without human intervention.\n",
            "[01:08.700 --> 01:15.400]  Of course, that kind of independence comes at the cost of having a much higher volume of data to train our machine.\n",
            "[01:15.400 --> 01:19.200]  Now, let's dive into the working of neural networks.\n",
            "[01:19.300 --> 01:21.200]  Here we have three students.\n",
            "[01:21.200 --> 01:24.900]  Each of them write down the digit 9 on a piece of paper.\n",
            "[01:24.900 --> 01:27.900]  Notably, they don't all write it identically.\n",
            "[01:27.900 --> 01:31.200]  The human brain can easily recognize the digits.\n",
            "[01:31.200 --> 01:34.700]  But what if a computer had to recognize them?\n",
            "[01:34.700 --> 01:37.200]  That's where deep learning comes in.\n",
            "[01:37.200 --> 01:41.100]  Here's a neural network trained to identify handwritten digits.\n",
            "[01:41.100 --> 01:45.700]  Each number is present as an image of 28 x 28 pixels.\n",
            "[01:45.700 --> 01:50.000]  Now, that amounts to a total of 784 pixels.\n",
            "[01:50.000 --> 01:56.100]  Neurons, the core entity of a neural network, is where the information processing takes place.\n",
            "[01:56.100 --> 02:02.300]  Each of the 784 pixels is fed to a neuron in the first layer of our neural network.\n",
            "[02:02.300 --> 02:04.900]  This forms the input layer.\n",
            "[02:04.900 --> 02:12.400]  On the other end, we have the output layer with each neuron representing a digit with the hidden layers existing between them.\n",
            "[02:12.400 --> 02:17.500]  The information is transferred from one layer to another over connecting channels.\n",
            "[02:17.500 --> 02:22.600]  Each of these has a value attached to it and hence is called a weighted channel.\n",
            "[02:22.600 --> 02:27.300]  All neurons have a unique number associated with it called bias.\n",
            "[02:27.300 --> 02:31.900]  This bias is added to the weighted sum of inputs reaching the neuron,\n",
            "[02:31.900 --> 02:36.900]  which is then applied to a function known as the activation function.\n",
            "[02:36.900 --> 02:41.800]  The result of the activation function determines if the neuron gets activated.\n",
            "[02:41.800 --> 02:46.700]  Every activated neuron passes on information to the following layers.\n",
            "[02:46.700 --> 02:50.300]  This continues up till the second last layer.\n",
            "[02:50.300 --> 02:55.500]  The one neuron activated in the output layer corresponds to the input digit.\n",
            "[02:55.500 --> 03:00.500]  The weights and bias are continuously adjusted to produce a well-trained network.\n",
            "[03:00.500 --> 03:02.900]  So, where is deep learning applied?\n",
            "[03:02.900 --> 03:04.400]  In customer support.\n",
            "[03:04.400 --> 03:09.900]  When most people converse with customer support agents, the conversation seems so real.\n",
            "[03:10.000 --> 03:13.800]  They don't even realize that it's actually a bot on the other side.\n",
            "[03:13.800 --> 03:20.900]  In medical care, neural networks detect cancer cells and analyze MRI images to give detailed results.\n",
            "[03:20.900 --> 03:22.800]  Self-driving cars.\n",
            "[03:22.800 --> 03:26.700]  What seemed like science fiction is now a reality.\n",
            "[03:26.700 --> 03:32.200]  Apple, Tesla and Nissan are only a few of the companies working on self-driving cars.\n",
            "[03:32.200 --> 03:37.900]  So, deep learning has a vast scope, but it too faces some limitations.\n",
            "[03:37.900 --> 03:41.000]  The first, as we discussed earlier, is data.\n",
            "[03:41.000 --> 03:45.300]  While deep learning is the most efficient way to deal with unstructured data,\n",
            "[03:45.300 --> 03:49.200]  a neural network requires a massive volume of data to train.\n",
            "[03:49.200 --> 03:53.200]  Let's assume we always have access to the necessary amount of data.\n",
            "[03:53.200 --> 03:57.400]  Processing this is not within the capability of every machine.\n",
            "[03:57.400 --> 04:00.500]  And that brings us to our second limitation.\n",
            "[04:00.500 --> 04:02.200]  Computational power.\n",
            "[04:02.200 --> 04:06.500]  Training a neural network requires graphical processing units,\n",
            "[04:06.600 --> 04:10.400]  which have thousands of cores as compared to CPUs.\n",
            "[04:10.400 --> 04:14.000]  And GPUs are of course more expensive.\n",
            "[04:14.000 --> 04:17.200]  And finally, we come down to training time.\n",
            "[04:17.200 --> 04:21.300]  Deep neural networks take hours or even months to train.\n",
            "[04:21.300 --> 04:26.200]  The time increases with the amount of data and number of layers in the network.\n",
            "[04:26.200 --> 04:28.600]  So, here's a short quiz for you.\n",
            "[04:28.600 --> 04:33.700]  Arrange the following statements in order to describe the working of a neural network.\n",
            "[04:33.700 --> 04:35.900]  A. The bias is added.\n",
            "[04:35.900 --> 04:39.500]  B. The weighted sum of the inputs is calculated.\n",
            "[04:39.500 --> 04:42.700]  C. Specific neuron is activated.\n",
            "[04:42.700 --> 04:46.600]  D. The result is fed to an activation function.\n",
            "[04:46.600 --> 04:49.100]  Leave your answers in the comment section below.\n",
            "[04:49.100 --> 04:52.300]  Three of you stand a chance to win Amazon vouchers.\n",
            "[04:52.300 --> 04:53.100]  So hurry!\n",
            "[04:53.100 --> 04:56.500]  Some of the popular deep learning frameworks include TensorFlow,\n",
            "[04:56.500 --> 05:02.600]  PyTorch, Keras, Deep Learning 4j, Cafe, and Microsoft Cognitive Toolkit.\n",
            "[05:02.600 --> 05:06.600]  Considering the future predictions for deep learning and AI,\n",
            "[05:06.600 --> 05:09.000]  we seem to have only scratched the surface.\n",
            "[05:09.000 --> 05:12.800]  In fact, Horace Technology is working on a device for the blind\n",
            "[05:12.800 --> 05:17.500]  that uses deep learning with computer vision to describe the world to the users.\n",
            "[05:17.500 --> 05:24.600]  Replicating the human mind at the entirety may be not just an episode of science fiction for too long.\n",
            "[05:24.600 --> 05:27.700]  The future is indeed full of surprises.\n",
            "[05:27.700 --> 05:30.600]  And that is deep learning for you in short.\n",
            "[05:30.600 --> 05:33.400]  If you enjoyed this video, do like and share it.\n",
            "[05:33.400 --> 05:36.200]  Also, subscribe to our channel if you haven't yet,\n",
            "[05:36.200 --> 05:39.500]  as we have a lot more exciting videos coming up.\n",
            "[05:39.500 --> 05:41.100]  Fun learning till then!\n",
            "\n",
            "--- FIRST 5 SEGMENTS ---\n",
            "[0.00 - 5.80]: Ever wondered how Google translates an entire web page to a different language in a matter of seconds?\n",
            "[5.80 - 9.70]: Or your phone gallery group's images based on their location?\n",
            "[9.70 - 12.40]: All of this is a product of deep learning.\n",
            "[12.40 - 15.30]: But what exactly is deep learning?\n",
            "[15.30 - 21.10]: Deep learning is a subset of machine learning, which in turn is a subset of artificial intelligence.\n",
            "\n",
            "‚úÖ Transcription complete. Total segments: 83\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Step 3: Segmentation and Prompt Preparation\n",
        "The LLM cannot handle a single, massive block of text. We must break the transcript into smaller chunks (e.g., 5-minute segments) and ask the LLM to summarize each chunk.\n",
        "\n",
        "### Segment the Transcript"
      ],
      "metadata": {
        "id": "1sTaEmGRAwlh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to group transcription segments into larger chunks for the LLM\n",
        "def segment_transcript(segments, max_chunk_duration=300): # 300 seconds = 5 minutes\n",
        "    chunks = []\n",
        "    current_chunk = []\n",
        "    current_duration = 0\n",
        "\n",
        "    if not segments:\n",
        "        return chunks\n",
        "\n",
        "    current_start = segments[0]['start']\n",
        "\n",
        "    for seg in segments:\n",
        "        segment_duration = seg['end'] - current_start\n",
        "\n",
        "        if segment_duration > max_chunk_duration and current_chunk:\n",
        "            # Finalize the current chunk\n",
        "            chunks.append({\n",
        "                'start': current_start,\n",
        "                'end': current_chunk[-1]['end'],\n",
        "                'text': \" \".join([s['text'] for s in current_chunk])\n",
        "            })\n",
        "            # Start a new chunk\n",
        "            current_chunk = [seg]\n",
        "            current_start = seg['start']\n",
        "        else:\n",
        "            current_chunk.append(seg)\n",
        "\n",
        "    # Add the last chunk\n",
        "    if current_chunk:\n",
        "        chunks.append({\n",
        "            'start': current_start,\n",
        "            'end': current_chunk[-1]['end'],\n",
        "            'text': \" \".join([s['text'] for s in current_chunk])\n",
        "        })\n",
        "\n",
        "    return chunks\n",
        "\n",
        "# Run the segmentation on your Whisper output\n",
        "llm_chunks = segment_transcript(transcript_segments, max_chunk_duration=300)\n",
        "\n",
        "print(f\"Video length: {transcript_segments[-1]['end'] / 60:.2f} minutes\")\n",
        "print(f\"Divided into {len(llm_chunks)} LLM chunks.\")\n",
        "print(f\"Example Chunk 1 Start: {llm_chunks[0]['start']:.2f}, End: {llm_chunks[0]['end']:.2f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yaL60jdNA2Wh",
        "outputId": "e55ed8f5-58fe-4898-969a-67ca8a2f863f"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Video length: 5.69 minutes\n",
            "Divided into 2 LLM chunks.\n",
            "Example Chunk 1 Start: 0.00, End: 296.50\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "U6wiKHB5BBDQ"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Step 4: Summarization with Gemini (The AI Core)\n",
        "We will use the Gemini API to perform high-quality, abstractive summarization on each chunk. You need to set your API key here.\n",
        "\n",
        "### LLM Summarization"
      ],
      "metadata": {
        "id": "brdOiurYBHoh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# UNINSTALL the old, deprecated SDK\n",
        "!pip uninstall -y google-generativeai\n",
        "\n",
        "# INSTALL the new, official, actively maintained SDK\n",
        "!pip install -q -U google-genai"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "srwcmnDbDgzK",
        "outputId": "3750d5c8-47ba-4d73-93ca-5b2aca59002e"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[33mWARNING: Skipping google-generativeai as it is not installed.\u001b[0m\u001b[33m\n",
            "\u001b[0m"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google import genai\n",
        "from google.genai import types\n",
        "\n",
        "API_KEY = \"<Your API Key>\"\n",
        "client = genai.Client(api_key=API_KEY)\n",
        "\n",
        "SYSTEM_PROMPT = (\n",
        "    \"You are an expert archivist generating a detailed, continuous text description \"\n",
        "    \"of a video's content based on its transcript. Your summary must be abstractive, \"\n",
        "    \"highly detailed, and strictly confined to the time segment provided. \"\n",
        "    \"Focus on key ideas, topics, and sequence of events. Do NOT use markdown headings or bullet points.\"\n",
        ")\n",
        "\n",
        "final_summary_data = []\n",
        "\n",
        "for i, chunk in enumerate(llm_chunks):\n",
        "    start_time_min = chunk['start'] / 60\n",
        "    end_time_min = chunk['end'] / 60\n",
        "    print(f\"Processing Chunk {i+1}/{len(llm_chunks)}: [{start_time_min:.2f}m - {end_time_min:.2f}m]...\")\n",
        "\n",
        "    user_message = (\n",
        "        f\"{SYSTEM_PROMPT}\\n\\n\"\n",
        "        f\"The video segment runs from minute {start_time_min:.2f} to minute {end_time_min:.2f}. \"\n",
        "        f\"Generate a detailed narrative description of this segment based on the following transcript:\\n\\n---\\n{chunk['text']}\"\n",
        "    )\n",
        "\n",
        "    try:\n",
        "        chat = client.chats.create(\n",
        "            model=\"gemini-2.5-flash\",\n",
        "            history=[\n",
        "                types.Content(role=\"user\", parts=[types.Part(text=user_message)])\n",
        "            ]\n",
        "        )\n",
        "        response = chat.send_message(\"Provide the summary.\")\n",
        "        summary_text = response.text.strip()\n",
        "\n",
        "        final_summary_data.append({\n",
        "            'start_s': chunk['start'],\n",
        "            'end_s': chunk['end'],\n",
        "            'start_m': start_time_min,\n",
        "            'end_m': end_time_min,\n",
        "            'description': summary_text\n",
        "        })\n",
        "\n",
        "        print(f\"Chunk {i+1} summarized. (Output length: {len(summary_text)} chars)\")\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"üö® Error processing chunk {i+1}: {e}\")\n",
        "        final_summary_data.append({\n",
        "            'start_s': chunk['start'],\n",
        "            'end_s': chunk['end'],\n",
        "            'start_m': start_time_min,\n",
        "            'end_m': end_time_min,\n",
        "            'description': \"ERROR: Could not generate summary for this segment.\"\n",
        "        })\n",
        "\n",
        "print(\"‚úÖ LLM Summarization complete.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eQ1vMPxKBPuJ",
        "outputId": "6da9c32a-2c10-4fae-8c33-6f2459140a94"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing Chunk 1/2: [0.00m - 4.94m]...\n",
            "Chunk 1 summarized. (Output length: 3588 chars)\n",
            "Processing Chunk 2/2: [4.94m - 5.69m]...\n",
            "Chunk 2 summarized. (Output length: 987 chars)\n",
            "‚úÖ LLM Summarization complete.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Step 5: Final Output and Archival\n",
        "This step stitches all the individual LLM summaries together into a final, coherent document.\n",
        "\n",
        "### Final Output Generation"
      ],
      "metadata": {
        "id": "U66ur9MIHrAD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import datetime\n",
        "# --- FUNCTION TO FORMAT TIME ---\n",
        "def format_time_hms(seconds):\n",
        "    hours, remainder = divmod(int(seconds), 3600)\n",
        "    minutes, seconds = divmod(remainder, 60)\n",
        "    return f\"{hours:02d}:{minutes:02d}:{seconds:02d}\"\n",
        "\n",
        "# --- GENERATE FINAL TEXT REPORT ---\n",
        "final_report_lines = [\n",
        "    f\"VIDEO FOOTAGE DETAILED TEXT ARCHIVE\",\n",
        "    f\"Source: {youtube_url}\",\n",
        "    f\"Generated on: {datetime.datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\",\n",
        "    \"-\" * 60,\n",
        "    \"\\n\"\n",
        "]\n",
        "for item in final_summary_data:\n",
        "    start_time_str = format_time_hms(item['start_s'])\n",
        "    end_time_str = format_time_hms(item['end_s'])\n",
        "\n",
        "    # Format the detailed, time-stamped description\n",
        "    summary_line = f\"[{start_time_str} - {end_time_str}]: {item['description']}\\n\"\n",
        "    final_report_lines.append(summary_line)\n",
        "\n",
        "final_report_text = \"\\n\".join(final_report_lines)\n",
        "\n",
        "# --- DISPLAY AND SAVE ---\n",
        "print(\"\\n\" + \"=\" * 60)\n",
        "print(\"             FINAL DETAILED TEXT SUMMARY\")\n",
        "print(\"=\" * 60)\n",
        "print(final_report_text)\n",
        "print(\"=\" * 60)\n",
        "\n",
        "# Save to a file (the small text file you wanted!)\n",
        "archive_filename = \"video_summary_archive.txt\"\n",
        "with open(archive_filename, \"w\", encoding=\"utf-8\") as f:\n",
        "    f.write(final_report_text)\n",
        "\n",
        "# Download the file from Colab\n",
        "from google.colab import files\n",
        "files.download(archive_filename)\n",
        "\n",
        "print(f\"\\n‚úÖ SUCCESS! Your detailed text archive file ({archive_filename}) is ready and downloaded.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 471
        },
        "id": "yhIqCqVxH0Mw",
        "outputId": "cb20c8b1-6154-4a4b-9b3b-bad54ede9c29"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "============================================================\n",
            "             FINAL DETAILED TEXT SUMMARY\n",
            "============================================================\n",
            "VIDEO FOOTAGE DETAILED TEXT ARCHIVE\n",
            "Source: https://www.youtube.com/watch?v=6M5VXKLf4D4\n",
            "Generated on: 2025-10-15 18:27:46\n",
            "------------------------------------------------------------\n",
            "\n",
            "\n",
            "[00:00:00 - 00:04:56]: The video segment commences by illustrating the practical applications of deep learning through common examples such as Google's real-time web page translation and phone galleries organizing images by location. It then systematically defines deep learning, positioning it as a subset of machine learning, which itself is a subset of artificial intelligence. Artificial intelligence is characterized as a technique enabling machines to mimic human behavior, while machine learning is a method to achieve AI by training algorithms with data. Deep learning, specifically, is described as a type of machine learning inspired by the human brain's structure, termed an artificial neural network.\n",
            "\n",
            "A detailed comparison between deep learning and machine learning is provided using the scenario of a machine differentiating between tomatoes and cherries. With traditional machine learning, humans must explicitly define differentiating features like size and stem type. In contrast, deep learning's neural networks autonomously identify these features without human intervention, though this independence necessitates a substantially higher volume of training data.\n",
            "\n",
            "The explanation then transitions into the intricate working of neural networks, utilizing the example of identifying handwritten digits. Each digit is presented as a 28x28 pixel image, totaling 784 pixels. Each of these pixels feeds into a neuron in the neural network's input layer. The network also comprises an output layer, where each neuron represents a specific digit, and multiple hidden layers situated in between. Information is transmitted between layers via connecting channels, each possessing an associated \"weight,\" thus termed weighted channels. Every neuron also has a unique \"bias\" value. The bias is added to the weighted sum of inputs received by a neuron, and this combined value is then processed by an activation function. The outcome of this function determines if the neuron \"activates,\" passing information to subsequent layers until the final output layer is reached, where the activated neuron indicates the recognized digit. Throughout this process, weights and biases are continuously adjusted to ensure the network is well-trained.\n",
            "\n",
            "The video then highlights several key applications of deep learning. These include customer support, where bots achieve highly realistic conversations, and medical care, where neural networks aid in detecting cancer cells and analyzing MRI images for detailed diagnostic results. Furthermore, the segment points to self-driving cars, once considered science fiction, as a current reality, citing companies like Apple, Tesla, and Nissan as major contributors.\n",
            "\n",
            "Despite its vast scope, deep learning faces several limitations. The primary challenge is the immense volume of data required for training neural networks, particularly for unstructured data, as processing such large datasets demands significant computational power. This leads to the second limitation: the need for powerful hardware, specifically Graphical Processing Units (GPUs) with thousands of cores, which are considerably more expensive than CPUs. Finally, the training time for deep neural networks is a significant constraint, often ranging from hours to several months, directly correlating with the amount of data and the number of layers within the network. The segment briefly concludes with a quiz asking viewers to order the steps of a neural network's operation, offering Amazon vouchers as an incentive for participation, and mentions popular deep learning frameworks such as TensorFlow.\n",
            "\n",
            "[00:04:56 - 00:05:41]: The segment begins by listing several prominent deep learning frameworks, including PyTorch, Keras, Deep Learning 4j, Cafe, and Microsoft Cognitive Toolkit. The discussion then shifts to the future of deep learning and AI, with the speaker emphasizing that humanity has only begun to explore its full potential. A specific example of innovative future application is provided: Horace Technology's development of a device for the blind that leverages deep learning and computer vision to describe the surrounding world to its users. This leads to a broader speculation that replicating the human mind in its entirety may soon transcend the realm of science fiction, highlighting that the future is anticipated to bring many unexpected developments. The speaker then concisely concludes the explanation of deep learning, transitioning into a call to action by encouraging viewers to like, share, and subscribe to the channel for more upcoming videos, wishing them continued \"fun learning.\"\n",
            "\n",
            "============================================================\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_ba7247f3-1636-4a87-aeaf-ce90f020e63d\", \"video_summary_archive.txt\", 4809)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "‚úÖ SUCCESS! Your detailed text archive file (video_summary_archive.txt) is ready and downloaded.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "xAaE1q8bH727"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}